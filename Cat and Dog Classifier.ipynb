{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training multiple models at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\\\Downloads\\\\Datasets\\\\PetImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Dog', 'Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing all the images\n",
    "img_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training data\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in categories:\n",
    "        path = os.path.join(data_dir, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE) # Converts image to array, second parameter makes grayscale\n",
    "                new_array = cv2.resize(img_array, (img_size, img_size)) \n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "create_training_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data so the list isn't all dogs then all cats\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning data to X and y variables\n",
    "X = []\n",
    "y = []\n",
    "for feature, target in training_data:\n",
    "    X.append(feature)\n",
    "    y.append(target)\n",
    "X = np.array(X).reshape(-1, img_size, img_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data so we don't have to reprocess everytime\n",
    "import pickle\n",
    "\n",
    "pickle_out = open('X.pickle', 'wb') #Learn about Pickle\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y.pickle', 'wb')\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data back\n",
    "pickle_in = open('X.pickle', 'rb')\n",
    "X = pickle.load(pickle_in)\n",
    "y = pickle.load(open('y.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a convolutional neural network to detect images\n",
    "X = X/255. #Scaling the inputs to be floats rather than ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 5s 229us/sample - loss: 0.6139 - accuracy: 0.6637 - val_loss: 0.5682 - val_accuracy: 0.7142\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 4s 185us/sample - loss: 0.5282 - accuracy: 0.7395 - val_loss: 0.5264 - val_accuracy: 0.7511\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 4s 187us/sample - loss: 0.4875 - accuracy: 0.7646 - val_loss: 0.4831 - val_accuracy: 0.7739\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 4s 177us/sample - loss: 0.4552 - accuracy: 0.7854 - val_loss: 0.4925 - val_accuracy: 0.7715\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 4s 185us/sample - loss: 0.4330 - accuracy: 0.8012 - val_loss: 0.5222 - val_accuracy: 0.7547\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 4s 188us/sample - loss: 0.4131 - accuracy: 0.8100 - val_loss: 0.4776 - val_accuracy: 0.7792\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 4s 189us/sample - loss: 0.3913 - accuracy: 0.8205 - val_loss: 0.4566 - val_accuracy: 0.7960\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 4s 187us/sample - loss: 0.3705 - accuracy: 0.8318 - val_loss: 0.4676 - val_accuracy: 0.7864\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 4s 200us/sample - loss: 0.3554 - accuracy: 0.8413 - val_loss: 0.4799 - val_accuracy: 0.7892\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 4s 189us/sample - loss: 0.3418 - accuracy: 0.8490 - val_loss: 0.4653 - val_accuracy: 0.7964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2182a78ef48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape=X.shape[1:])) #First convolution layer; (3,3) is the window size\n",
    "model.add(Activation('relu')) #Activation layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) #First pooling layer\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten()) #Need to flatten data because last layer needs 1D input\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1)) #Output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "y = np.array(y).reshape(-1,1)\n",
    "model.fit(X,y,batch_size=32, epochs=10,validation_split=0.1) #Batch size: how many data points you're feeding in at a time\n",
    "                                                   #Validation split is size of your test data split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
